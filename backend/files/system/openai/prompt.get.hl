
// Asks OpenAI a question with the specified type, and returns the answer to caller.
.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   session:string
.description:Asks OpenAI a question with the specified type, and returns the answer to caller
.type:public

// Invoking slot containing commonalities for all endpoints.
add:x:./*/signal
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Checking if request has been previously cached.
      data.read
         table:ml_requests
         columns
            completion
            cached
         where
            and
               prompt.eq:x:@.arguments/*/prompt
               type.eq:x:@.arguments/*/type
         limit:1
         order:created
         direction:desc

      // Checking if we could find a matching request in cache.
      if
         and
            exists:x:@data.read/*/*
            eq
               convert:x:@data.read/*/*/cached
                  type:int
               .:int:1
         .lambda

            // Checking if type is cached.
            if
               and
                  exists:x:@.arguments/*/cached
                  not-null:x:@.arguments/*/cached
                  eq
                     convert:x:@.arguments/*/cached
                        type:int
                     .:int:1
               .lambda

                  // Returning cached completion.
                  unwrap:x:+/*
                  return
                     result:x:@data.read/*/*/completion
                     finish_reason:cached

      /*
       * Making sure we always have room for context in case model has been configured
       * to spend all tokens on its reply by making sure max_tokens never exceed 50%
       * of the model's capacity.
       */
      switch:x:@.arguments/*/model

         case:gpt-3.5-turbo
         case:gpt-3.5-turbo-0301

            if
               mt
                  convert:x:@.arguments/*/max_tokens
                     type:int
                  .:int:1000
               .lambda
                  set-value:x:@.arguments/*/max_tokens
                     .:int:1000

         case:text-davinci-003
         case:text-davinci-002
         case:code-davinci-002

            if
               mt
                  convert:x:@.arguments/*/max_tokens
                     type:int
                  .:int:2000
               .lambda
                  set-value:x:@.arguments/*/max_tokens
                     .:int:2000

         default

            if
               mt
                  convert:x:@.arguments/*/max_tokens
                     type:int
                  .:int:1000
               .lambda
                  set-value:x:@.arguments/*/max_tokens
                     .:int:1000

      // Invoking Hyperlambda file doing the heavy lifting.
      .result
      if
         eq
            convert:x:@.arguments/*/use_embeddings
               type:int
            .:int:1
         .lambda

            // Using embeddings API.
            add:x:./*/io.file.execute
               get-nodes:x:@.arguments/*/type
               get-nodes:x:@.arguments/*/chat
               get-nodes:x:@.arguments/*/prompt
               get-nodes:x:@.arguments/*/session
               get-nodes:x:@.arguments/*/references
               get-nodes:x:@.arguments/*/prefix
               get-nodes:x:@.arguments/*/model
               get-nodes:x:@.arguments/*/max_tokens
               get-nodes:x:@.arguments/*/temperature
               get-nodes:x:@.arguments/*/threshold
               get-nodes:x:@.arguments/*/vector_model
               .
                  fine_tuned:bool:false
            io.file.execute:/system/openai/prompt.implementation/embeddings-prompt.hl
            add:x:@.result
               get-nodes:x:@io.file.execute/*

      else

         // Not using embeddings.
         add:x:./*/io.file.execute
            get-nodes:x:@.arguments/*/type
            get-nodes:x:@.arguments/*/prompt
            get-nodes:x:@.arguments/*/session
            get-nodes:x:@.arguments/*/prefix
            get-nodes:x:@.arguments/*/model
            get-nodes:x:@.arguments/*/max_tokens
            get-nodes:x:@.arguments/*/temperature
            .
               fine_tuned:bool:true
         io.file.execute:/system/openai/prompt.implementation/completion-prompt.hl
         add:x:@.result
            get-nodes:x:@io.file.execute/*

      // Checking if type is 'supervised', at which point we store prompt and completion.
      if
         and
            not
               get-value:x:@.result/*/error
            not
               exists:x:@data.read/*/*
            mt
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:0
         .lambda

            // Storing prompt and completion in ml_requests table.
            data.create
               table:ml_requests
               values
                  type:x:@.arguments/*/type
                  prompt:x:@.arguments/*/prompt
                  completion:x:@.result/*/result
                  finish_reason:x:@.result/*/finish_reason

      /*
       * Applying some HTTP caching to avoid invoking OpenAI again with
       * the same question before some minimum amount of time has passed.
       */
      response.headers.set
         Cache-Control:max-age=30

      // Returning results returned from invocation above to caller.
      add:x:+
         get-nodes:x:@.result/*/result
         get-nodes:x:@.result/*/finish_reason
         get-nodes:x:@.result/*/references
      return

// Returning result of worker slot to caller.
return-nodes:x:@signal/*
