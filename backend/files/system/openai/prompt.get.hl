
// Asks OpenAI a question with the specified type, and returns the answer to caller.
.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   session:string
.description:Asks OpenAI a question with the specified type, and returns the answer to caller
.type:public

// Invoking slot containing commonalities for all endpoints.
add:x:./*/signal
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Checking if request has been previously cached.
      data.read
         table:ml_requests
         columns
            completion
            cached
         where
            and
               prompt.eq:x:@.arguments/*/prompt
               type.eq:x:@.arguments/*/type
         limit:1
         order:created
         direction:desc

      // Checking if we could find a matching request in cache.
      if
         and
            exists:x:@data.read/*/*
            eq
               convert:x:@data.read/*/*/cached
                  type:int
               .:int:1
         .lambda

            // Checking if type is cached.
            if
               and
                  exists:x:@.arguments/*/cached
                  not-null:x:@.arguments/*/cached
                  eq
                     convert:x:@.arguments/*/cached
                        type:int
                     .:int:1
               .lambda

                  // Returning cached completion.
                  unwrap:x:+/*
                  return
                     result:x:@data.read/*/*/completion
                     finish_reason:cached

      // Result we're returning to caller.
      .result

      // Creating our decorated prompt.
      .prompt
      if
         eq
            convert:x:@.arguments/*/use_embeddings
               type:int
            .:int:1
         .lambda

            // Retrieving relevant snippets.
            unwrap:x:+/*
            signal:magic.ai.get-context
               type:x:@.arguments/*/type
               prompt:x:@.arguments/*/prompt
               threshold:x:@.arguments/*/threshold
               max_tokens:x:@.arguments/*/max_context_tokens
               vector_model:x:@.arguments/*/vector_model

            // Correctly applying prefix.
            if
               and
                  strings.contains:x:@.arguments/*/prefix
                     .:[CONTEXT]
                  strings.contains:x:@.arguments/*/prefix
                     .:[QUESTION]
                  strings.contains:x:@.arguments/*/prefix
                     .:"ANSWER:"
               .lambda

                  // Structured prefix.
                  set-value:x:@.prompt
                     strings.replace:x:@.arguments/*/prefix
                        .:[QUESTION]
                        get-value:x:@.arguments/*/prompt
                  set-value:x:@.prompt
                     strings.replace:x:@.prompt
                        .:[CONTEXT]
                        get-value:x:@signal/*/context

            else

               // Old style prefix.
               set-value:x:@.prompt
                  strings.concat
                     get-value:x:@.arguments/*/prefix
                     .:"\r\nQUESTION: "
                     get-value:x:@.arguments/*/prompt
                     .:"\r\nCONTEXT: \r\n"
                     get-value:x:@signal/*/context
                     .:"\r\nANSWER: "

            // Checking if caller wants references.
            if
               and
                  exists:x:@.arguments/*/references
                  not-null:x:@.arguments/*/references
                  get-value:x:@.arguments/*/references
               .lambda
                  add:x:@.result
                     .
                        references
                  add:x:@.result/*/references
                     get-nodes:x:@signal/*/snippets/*

      else

         // Not using embeddings, either a mint model or a fine-tuned model.
         set-value:x:@.prompt
            strings.concat
               get-value:x:@.arguments/*/prompt
               .:" ->"
         add:x:@.callback/*/http.post/*/payload
            .
               stop:" END"

      // Retrieving token.
      .token
      set-value:x:@.token
         strings.concat
            .:"Bearer "
            config.get:"magic:openai:key"

      // Invoking OpenAI, now with a decorated prompt.
      http.post:"https://api.openai.com/v1/completions"
         headers
            Authorization:x:@.token
            Content-Type:application/json
         payload
            prompt:x:@.prompt
            model:x:@.arguments/*/model
            max_tokens:x:@.arguments/*/max_tokens
            temperature:x:@.arguments/*/temperature
         convert:true

      // Sanity checking above invocation
      if
         not
            and
               mte:x:@http.post
                  .:int:200
               lt:x:@http.post
                  .:int:300
         .lambda

            // Oops, error - Logging error and returning status 500 to caller.
            lambda2hyper:x:@http.post
            log.error:Something went wrong while invoking OpenAI
               message:x:@http.post/*/content/*/error/*/message
               status:x:@http.post
               error:x:@lambda2hyper
            response.status.set:x:@http.post
            unwrap:x:+/*
            return
               error:bool:true
               result:x:@http.post/*/content/*/error/*/message
      else

         // Success! Logging as such!
         log.info:Invoking OpenAI was a success

      // Making sure we trim response before adding it to [.result].
      strings.trim:x:@http.post/*/content/*/choices/0/*/text
      unwrap:x:+/*/*
      add:x:@.result
         .
            result:x:@strings.trim

      // Making sure we return finish reason to caller.
      get-first-value
         get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
         .:unknown
      unwrap:x:+/*/*
      add:x:@.result
         .
            finish_reason:x:@get-first-value

      // Checking if type is 'supervised', at which point we store prompt and completion.
      if
         and
            not
               get-value:x:@.result/*/error
            not
               exists:x:@data.read/*/*
            mt
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:0
         .lambda

            // Storing prompt and completion in ml_requests table.
            data.create
               table:ml_requests
               values
                  type:x:@.arguments/*/type
                  prompt:x:@.arguments/*/prompt
                  completion:x:@.result/*/result
                  finish_reason:x:@.result/*/finish_reason

      /*
       * Applying some HTTP caching to avoid invoking OpenAI again with
       * the same question before some minimum amount of time has passed.
       */
      response.headers.set
         Cache-Control:max-age=30

      // Returning results returned from invocation above to caller.
      add:x:+
         get-nodes:x:@.result/*/result
         get-nodes:x:@.result/*/finish_reason
         get-nodes:x:@.result/*/references
      return

// Returning result of worker slot to caller.
return-nodes:x:@signal/*
