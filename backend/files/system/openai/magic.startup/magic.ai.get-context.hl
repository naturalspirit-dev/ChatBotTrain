
/*
 * Slot that returns context and references for the specified [type], given the
 * specified [threshold], the specified [prompt], the specified [vector_model]
 * and the specified [max_tokens].
 */
slots.create:magic.ai.get-context

   // Sanity checking invocation.
   validators.mandatory:x:@.arguments/*/type
   validators.mandatory:x:@.arguments/*/prompt
   validators.mandatory:x:@.arguments/*/threshold
   validators.mandatory:x:@.arguments/*/max_tokens
   validators.mandatory:x:@.arguments/*/vector_model
   validators.string:x:@.arguments/*/prompt
      min:1

   // Converting threshold in case we're given the wrong type.
   set-value:x:@.arguments/*/threshold
      convert:x:@.arguments/*/threshold
         type:double

   // Retrieving embeddings.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         config.get:"magic:openai:key"

   // Retrieving embedding for prompt.
   http.post:"https://api.openai.com/v1/embeddings"
      headers
         Authorization:x:@.token
         Content-Type:application/json
      payload
         input:x:@.arguments/*/prompt
         model:x:@.arguments/*/vector_model
      convert:true

   // Sanity checking above invocation.
   if
      not
         and
            mte:x:@http.post
               .:int:200
            lt:x:@http.post
               .:int:300
      .lambda

         // Oops, error - Logging error and returning OpenAI's HTTP status code to caller.
         lambda2hyper:x:@http.post
         log.error:Something went wrong while invoking OpenAI
            message:x:@http.post/*/content/*/error/*/message
            error:x:@lambda2hyper
         throw:x:@http.post/*/content/*/error/*/message
            public:bool:true
            status:x:@http.post

   // Fetching relevant snippets, making sure we profile the time it takes.
   .begin
   set-value:x:@.begin
      date.now

   // Connecting to database to retrieve embeddings that are related to prompt.
   .scan
   data.connect:[generic|magic]

      // Converting embeddings to a byte array of floats, since this is how we store embeddings in SQLite.
      floatArray2bytes:x:@http.post/*/content/*/data/0/*/embedding/*
      math.subtract
         .:float:1
         get-value:x:@.arguments/*/threshold
      data.select:@"
select vss.distance, vss.rowid as id, ts.prompt, ts.completion, ts.uri, ts.cached
	from vss_ml_training_snippets as vss
    	inner join ml_training_snippets ts on ts.id = vss.rowid
   where vss_search(
     vss.embedding_vss,
     vss_search_params(@embedding, 20)
   ) and vss.distance < @threshold"
         embedding:x:@floatArray2bytes
         type:x:@.arguments/*/type
         threshold:x:@math.subtract
      add:x:@.scan
         get-nodes:x:@data.select/*

   /*
    * Checking if first snippet is cached, at which point we return ONLY that snippet.
    *
    * This allows you to have responses that are statically cached, yet still
    * dependent upon semantic AI search towards your training snippets.
    */
   if
      and
         exists:x:@.scan/0
         not-null:x:@.scan/0/*/cached
         mt
            convert:x:@.scan/0/*/cached
               type:int
            .:int:0
      .lambda

         // First matching snippet has been cached.
         unwrap:x:+/*
         return
            cached:x:@.scan/0/*/completion

   /*
    * Used to hold the referenced snippets used to create our context.
    *
    * Returned to caller as [snippets].
    */
   .result

   // Context value we return to caller.
   .context:

   // Temporary variable used to calculate tokens.
   .tmp:

   /*
    * Running through all  creating our context
    * and storing referenced snippets in the process.
    */
   while
      exists:x:@.scan/0
      .lambda

         /*
          * Concatenating top snippet to [.tmp] buffer
          * such that we can calculate total number of tokens
          * resulting from adding currently iterated snippet.
          */
         set-value:x:@.tmp
            strings.concat
               get-value:x:@.tmp
               .:"\r\n"
               get-value:x:@.scan/0/*/prompt
               .:"\r\n"
               .:"\r\n"
               get-value:x:@.scan/0/*/completion
               .:"\r\n"

         /*
          * Verifying that adding currently iterated snippet doesn't
          * produce more tokens than acceptable.
          *
          * Notice, even if currently iterated snippet might overflow threshold,
          * it might be possible to add the NEXT snippet, with lower dot product score,
          * if it is smaller.
          */
         if
            lt
               openai.tokenize:x:@.tmp
               get-value:x:@.arguments/*/max_tokens
            .lambda

               /*
                * We've got room for more context, hence updating [.context]
                * to the value of [.tmp].
                */
               set-value:x:@.context
                  get-value:x:@.tmp

               /*
                * Checking if we can use currently iterated snippet as a "reference",
                * which is only true if the snippet actually contains a [uri], and has
                * not already been added as a reference.
                */
               if
                  and
                     not-null:x:@.scan/0/*/uri
                     not-exists:x:@.result/*/*/uri/={@.scan/0/*/uri}
                  .lambda

                     // Adding current reference to [.result]
                     unwrap:x:+/*/*/*
                     add:x:@.result
                        .
                           .
                              prompt:x:@.scan/0/*/prompt
                              uri:x:@.scan/0/*/uri
                              dot:x:@.scan/0/*/dot
         else

            /*
             * Removing the currently appended snippet from [.tmp]
             * to allow the logic to correctly check if we can use the NEXT snippet.
             */
            set-value:x:@.tmp
               get-value:x:@.context

         /*
          * Removing currently iterated snippet.
          *
          * Notice, this logic allows us to add smaller snippets with "less accuracy"
          * than larger snippets found as "better matches" since it will loop through
          * all 50 snippets we found above, and add "whatever it can add" to the context.
          */
         remove-nodes:x:@.scan/0

   // Measuring how much time we spent looping through snippets.
   .time
   set-value:x:@.time
      math.subtract
         date.now
         get-value:x:@.begin
   set-value:x:@.time
      time.format:x:@.time
         format:"ss\\.fff"

   // Trimming [.context] before we return it to caller.
   set-value:x:@.context
      strings.trim:x:@.context
         .:"\r\n \t"

   // Returning context and relevant snippets to caller.
   if
      and
         exists:x:@.context
         not-null:x:@.context
         neq:x:@.context
            .:
      .lambda
         add:x:+/+/*/*/snippets
            get-nodes:x:@.result/*
         unwrap:x:+/*/*
         add:x:../*/return
            .
               context:x:@.context
               snippets
   unwrap:x:./*/return/*
   return
      db_time:x:@.time
