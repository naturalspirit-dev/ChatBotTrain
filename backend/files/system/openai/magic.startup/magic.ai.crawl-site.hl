
/*
 * Crawls the specified website generating training data for machine learning in the process.
 */
slots.create:magic.ai.crawl-site

   // Creating a thread and invoking file doing the heavy lifting.
   insert-before:x:./*/fork/0
      get-nodes:x:@.arguments
   fork

      // Making sure exceptions does not leave thread.
      try

         /*
          * Checking if website has a sitemap first.
          * We do this by checking if we've got a "robots.txt" file, that contains a link
          * to the sitemap, and if so, using this as the URL for our sitemap. And if not,
          * defaulting the sitemap URL to "xyz.com/sitemap.xml" which is the default URL
          * for sitemaps.
          */
         .sitemap-url
         set-value:x:@.sitemap-url
            strings.concat
               get-value:x:@.arguments/*/url
               .:"/sitemap.xml"

         // Checking if site has a "robots.txt" file.
         .robots-url
         set-value:x:@.robots-url
            strings.concat
               get-value:x:@.arguments/*/url
               .:"/robots.txt"

         // Retrieving "robots.txt" file.
         http.get:x:@.robots-url
            headers
               User-Agent:AINIRO-MachineLearning-Spider

         // Checking if "robots.txt" exists.
         if
            and
               mte:x:@http.get
                  .:int:200
               lt:x:@http.get
                  .:int:300
               not-null:x:@http.get/*/content
               mt
                  strings.length:x:@http.get/*/content
                  .:int:0
            .lambda

               // Site has a "robots.txt" file.
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:Site has a robots.txt file, finding sitemap from it
                     type:info

               // Fiding sitemap's URL from robots.txt file
               strings.split:x:@http.get/*/content
                  .:"Sitemap:"
               if
                  mt
                     get-count:x:@strings.split/*
                     .:int:1
                  .lambda
                     strings.split:x:@strings.split/1
                        .:"\n"
                     strings.trim:x:@strings.split/0
                        .:"\r\n\t "
                     set-value:x:@.sitemap-url
                        get-value:x:@strings.trim

               // Verifying sitemap URL is a full URL, and if not, prepending [url] argument.
               if
                  not
                     strings.starts-with:x:@.sitemap-url
                        .:"http"
                  .lambda
                     set-value:x:@.sitemap-url
                        strings.concat
                           get-value:x:@.arguments/*/url
                           get-value:x:@.sitemap-url

               // Site has a "robots.txt" file.
               strings.concat
                  .:"Loading sitemap from "
                  get-value:x:@.sitemap-url
               unwrap:x:+/*/args/*/message
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:x:@strings.concat
                     type:info


         // Retrieving sitemap.
         http.get:x:@.sitemap-url
            headers
               User-Agent:AINIRO-MachineLearning-Spider

         // Checking if sitemap exists.
         if
            and
               mte:x:@http.get
                  .:int:200
               lt:x:@http.get
                  .:int:300
               not-null:x:@http.get/*/content
               mt
                  strings.length:x:@http.get/*/content
                  .:int:0
            .lambda

               // Site has a sitemap, ignoring HTML and processing sitemap.xml instead.
               log.info:Site contains a sitemap
                  url:x:@.arguments/*/url
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:Site contains a sitemap
                     type:info

               // Converting sitemap to XML.
               xml2lambda:x:@http.get/*/content

               // Used as buffer for URLs.
               .urls

               // Iterating through each URL in currently primary sitemap file.
               for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text

                  // Avoiding duplicates.
                  if
                     not-exists:x:@.urls/*/={@.dp/#}
                     .lambda
                        unwrap:x:+/*/*
                        add:x:@.urls
                           .
                              .:x:@.dp/#

               // Iterating through each CDATA URL in currently primary sitemap file.
               for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section

                  // Avoiding duplicates.
                  if
                     not-exists:x:@.urls/*/={@.dp/#}
                     .lambda
                        unwrap:x:+/*/*
                        add:x:@.urls
                           .
                              .:x:@.dp/#

               // Iterating through each URL referenced in main sitemap.
               for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#text

                  // Informing user.
                  strings.concat
                     .:"Parsing sitemap reference "
                     get-value:x:@.dp/#
                  unwrap:x:+/*/*/message
                  sockets.signal:magic.backend.chatbot
                     roles:root
                     args
                        message:x:@strings.concat
                        type:info

                  // Getting XML from currently iterated sitemap subset.
                  unwrap:x:+/*/*
                  http.get:x:@.dp/#
                     headers
                        User-Agent:AINIRO-MachineLearning-Spider

                  // Verifying above invocation returned success.
                  if
                     and
                        mte:x:@http.get
                           .:int:200
                        lt:x:@http.get
                           .:int:300
                     .lambda

                        // In case sitemap is malformed, we catch exceptions while trying to convert it to XML.
                        try

                           // Above invocation returned success.
                           xml2lambda:x:@http.get/*/content

                           // Iterating through each URL in currently iterated sitemap file.
                           for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text

                              // Avoiding duplicates.
                              if
                                 not-exists:x:@.urls/*/={@.dp/#}
                                 .lambda
                                    unwrap:x:+/*/*
                                    add:x:@.urls
                                       .
                                          .:x:@.dp/#

                        .catch

                           // Oops ...!!
                           log.error:Not able to parse sitemap reference as XML
                              url:x:@.dp/#
                           strings.concat
                              .:"Unable to parse "
                              get-value:x:@.dp/#
                              .:" as a valid XML document."
                           unwrap:x:+/*/*/message
                           sockets.signal:magic.backend.chatbot
                              roles:root
                              args
                                 message:x:@strings.concat
                                 type:warning

                  else

                     // Doing some logging.
                     log.error:Bogus sitemap reference
                        url:x:@.dp/#
                        primary-url:x:@.arguments/*/url

               // Iterating through each CDATA URL referenced in main sitemap.
               for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#cdata-section

                  // Getting XML from currently iterated sitemap subset.
                  unwrap:x:+/*/*
                  http.get:x:@.dp/#
                     headers
                        User-Agent:AINIRO-MachineLearning-Spider

                  // Verifying above invocation returned success.
                  if
                     and
                        mte:x:@http.get
                           .:int:200
                        lt:x:@http.get
                           .:int:300
                     .lambda

                        // Above invocation returned success.
                        xml2lambda:x:@http.get/*/content

                        // Iterating through each URL in currently iterated sitemap file.
                        for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section

                           // Avoiding duplicates.
                           if
                              not-exists:x:@.urls/*/={@.dp/#}
                              .lambda
                                 unwrap:x:+/*/*
                                 add:x:@.urls
                                    .
                                       .:x:@.dp/#

                        // Iterating through each URL referenced in iterated sitemap file.
                        for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#text

                           // Avoiding duplicates.
                           if
                              not-exists:x:@.urls/*/={@.dp/#}
                              .lambda
                                 unwrap:x:+/*/*
                                 add:x:@.urls
                                    .
                                       .:x:@.dp/#

                  else

                     // Doing some logging.
                     log.error:Bogus sitemap reference
                        url:x:@.dp/#
                        primary-url:x:@.arguments/*/url

               // Doing some logging.
               get-count:x:@.urls/*
               log.info:URLs found from sitemap(s)
                  url:x:@.arguments/*/url
                  urls:x:@get-count

               // Signaling frontend.
               strings.concat
                  .:"There are a total of "
                  get-value:x:@get-count
                  .:" pages in website"
               unwrap:x:+/*/args/*
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:x:@strings.concat
                     type:info

               // Checking if we need to warn user about maximum URLs.
               if
                  and
                     exists:x:@.arguments/*/max
                     mt
                        get-value:x:@get-count
                        get-value:x:@.arguments/*/max
                  .lambda

                     // Warning use that there are MORE than [max] URLs in sitemap.
                     strings.concat
                        .:"Warning, the site contains more than "
                        get-value:x:@.arguments/*/max
                        .:" pages, and will only be PARTIALLY scraped!"
                     unwrap:x:+/*/args/*/message
                     sockets.signal:magic.backend.chatbot
                        roles:root
                        args
                           message:x:@strings.concat
                           type:warning
                     log.info:x:@strings.concat

               // Sorting URLs by length, which typically makes sure more important URLs are prioritised.
               sort:x:@.urls/*
                  if
                     lt
                        strings.length:x:@.lhs/#
                        strings.length:x:@.rhs/#
                     .lambda
                        set-value:x:@.result
                           .:int:-1
                  else-if
                     mt
                        strings.length:x:@.lhs/#
                        strings.length:x:@.rhs/#
                     .lambda
                        set-value:x:@.result
                           .:int:1
                  else
                     set-value:x:@.result
                        .:int:0
               remove-nodes:x:@.urls/*
               add:x:@.urls
                  get-nodes:x:@sort/*

               // Looping through each URL found from sitemap(s).
               .no:int:0
               for-each:x:@.urls/*

                  // Checking if we've reached limit.
                  if
                     or
                        not-exists:x:@.arguments/*/max
                        lt:x:@.no
                           get-value:x:@.arguments/*/max
                     .lambda

                        // Informing frontend of what we're doing
                        strings.concat
                           .:"Crawling "
                           get-value:x:@.dp/#
                        unwrap:x:+/*/args/*
                        sockets.signal:magic.backend.chatbot
                           roles:root
                           args
                              message:x:@strings.concat
                              type:info

                        /*
                         * Invoking crawl implementation file, now with explicit list of links,
                         * informing invocation that we do NOT want to process links.
                         */
                        add:x:./*/io.file.execute
                           get-nodes:x:@.arguments/*/type
                           get-nodes:x:@.arguments/*/delay
                           get-nodes:x:@.arguments/*/max
                           get-nodes:x:@.arguments/*/threshold
                           get-nodes:x:@.arguments/*/summarize
                        unwrap:x:+/*/url
                        io.file.execute:/system/openai/crawl.implementation/import-url.hl
                           url:x:@.dp/#
                           crawl:bool:false

                        // Incrementing count of URLs we've processed.
                        math.increment:x:@.no

               // Signaling frontend
               strings.concat
                  .:"Done importing "
                  get-value:x:@.no
                  .:" URLs"
               unwrap:x:+/*/args/*
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:x:@strings.concat
                     type:done_crawling
               log.info:x:@strings.concat

         else

            /*
             * Site does not have a sitemap.xml, processing HTML instead,
             * crawling links found from HTML instead.
             */
            log.info:Site does not have a sitemap
               url:x:@.arguments/*/url
            add:x:./*/io.file.execute
               get-nodes:x:@.arguments/*
            io.file.execute:/system/openai/crawl.implementation/import-url.hl
               crawl:bool:true

         /*
          * Crawling is done.
          * Making sure we notify client that we're done and do some logging.
          */
         sockets.signal:magic.backend.message
            roles:root
            args
               message:Done creating OpenAI training data from URL
               type:success

         // Basic logging.
         log.info:OpenAI training data successfully created
            url:x:@.arguments/*/url
            type:x:@.arguments/*/type

         // Checking if caller wants us to execute some lambda object once we're done.
         if
            exists:x:@.arguments/*/.onafter
            .lambda
               eval:x:@.arguments/*/.onafter

      .catch

         // Oops ...!!
         log.error:x:@.arguments/*/message
            url:x:@.arguments/*/url

         // Signaling frontend.
         strings.concat
            .:"Error - "
            get-value:x:@.arguments/*/message
         unwrap:x:+/*/args/*
         sockets.signal:magic.backend.chatbot
            roles:root
            args
               message:x:@strings.concat
               type:error
