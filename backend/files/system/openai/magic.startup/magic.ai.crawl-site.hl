
/*
 * Crawls the specified website generating training data for machine learning in the process.
 */
slots.create:magic.ai.crawl-site

   // Creating a thread and invoking file doing the heavy lifting.
   insert-before:x:./*/fork/0
      get-nodes:x:@.arguments
   fork

      // Making sure exceptions does not leave thread.
      try

         // Checking if website has a sitemap.
         strings.concat
            get-value:x:@.arguments/*/url
            .:"/sitemap.xml"
         http.get:x:@strings.concat
            headers
               User-Agent:Aista-MachineLearning-Spider
         if
            and
               mte:x:@http.get
                  .:int:200
               lt:x:@http.get
                  .:int:300
            .lambda

               // Site has a sitemap, ignoring HTML and processing sitemap.xml instead.
               log.info:Site contains a sitemap
                  url:x:@.arguments/*/url
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:Site contains a sitemap
                     type:info

               // Converting sitemap to XML.
               xml2lambda:x:@http.get/*/content

               // Used as buffer for URLs.
               .urls

               // Iterating through each URL in currently primary sitemap file.
               for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text
                  unwrap:x:+/*/*
                  add:x:@.urls
                     .
                        .:x:@.dp/#

               // Iterating through each CDATA URL in currently primary sitemap file.
               for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section
                  unwrap:x:+/*/*
                  add:x:@.urls
                     .
                        .:x:@.dp/#

               // Iterating through each URL referenced in main sitemap.
               for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#text

                  // Getting XML from currently iterated sitemap subset.
                  unwrap:x:+/*/*
                  http.get:x:@.dp/#
                     headers
                        User-Agent:Aista-MachineLearning-Spider

                  // Verifying above invocation returned success.
                  if
                     and
                        mte:x:@http.get
                           .:int:200
                        lt:x:@http.get
                           .:int:300
                     .lambda

                        // Above invocation returned success.
                        xml2lambda:x:@http.get/*/content

                        // Iterating through each URL in currently iterated sitemap file.
                        for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text
                           unwrap:x:+/*/*
                           add:x:@.urls
                              .
                                 .:x:@.dp/#

                  else

                     // Doing some logging.
                     log.error:Bogus sitemap reference
                        url:x:@.dp/#
                        primary-url:x:@.arguments/*/url

               // Iterating through each CDATA URL referenced in main sitemap.
               for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#cdata-section

                  // Getting XML from currently iterated sitemap subset.
                  unwrap:x:+/*/*
                  http.get:x:@.dp/#
                     headers
                        User-Agent:Aista-MachineLearning-Spider

                  // Verifying above invocation returned success.
                  if
                     and
                        mte:x:@http.get
                           .:int:200
                        lt:x:@http.get
                           .:int:300
                     .lambda

                        // Above invocation returned success.
                        xml2lambda:x:@http.get/*/content

                        // Iterating through each URL in currently iterated sitemap file.
                        for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section
                           unwrap:x:+/*/*
                           add:x:@.urls
                              .
                                 .:x:@.dp/#

                  else

                     // Doing some logging.
                     log.error:Bogus sitemap reference
                        url:x:@.dp/#
                        primary-url:x:@.arguments/*/url

               // Doing some logging.
               get-count:x:@.urls/*
               log.info:URLs found from sitemap(s)
                  url:x:@.arguments/*/url
                  urls:x:@get-count

               // Signaling frontend.
               strings.concat
                  .:"There are "
                  get-value:x:@get-count
                  .:" URLs in total in site"
               unwrap:x:+/*/args/*
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:x:@strings.concat
                     type:info

               // Looping through each URL found from sitemap(s).
               .no:int:0
               for-each:x:@.urls/*

                  // Checking if we've reached limit.
                  if
                     lte:x:@.no
                        get-value:x:@.arguments/*/max
                     .lambda

                        // Informing frontend of what we're doing
                        strings.concat
                           .:"Processing "
                           get-value:x:@.dp/#
                        unwrap:x:+/*/args/*
                        sockets.signal:magic.backend.chatbot
                           roles:root
                           args
                              message:x:@strings.concat
                              type:info

                        /*
                         * Invoking crawl implementation file, now with explicit list of links,
                         * informing invocation that we do NOT want to process links.
                         */
                        add:x:./*/io.file.execute
                           get-nodes:x:@.arguments/*/type
                           get-nodes:x:@.arguments/*/delay
                           get-nodes:x:@.arguments/*/max
                           get-nodes:x:@.arguments/*/threshold
                        unwrap:x:+/*
                        io.file.execute:/system/openai/crawl.implementation/import-url.hl
                           url:x:@.dp/#
                           crawl:bool:false

                        // Incrementing count of URLs we've processed.
                        math.increment:x:@.no

               // Signaling frontend
               strings.concat
                  .:"Done importing "
                  get-value:x:@get-count
                  .:" URLs"
               unwrap:x:+/*/args/*
               sockets.signal:magic.backend.chatbot
                  roles:root
                  args
                     message:x:@strings.concat
                     type:info

         else

            // Site does not have a sitemap.xml, processing HTML instead.
            log.info:Site does not have a sitemap
               url:x:@.arguments/*/url
            add:x:./*/io.file.execute
               get-nodes:x:@.arguments/*
            io.file.execute:/system/openai/crawl.implementation/import-url.hl
               crawl:bool:true

         /*
          * Crawling is done.
          * Making sure we notify client that we're done and do some logging.
          */
         sockets.signal:magic.backend.message
            roles:root
            args
               message:Done creating OpenAI training data from URL
               type:success

         // Basic logging.
         log.info:OpenAI training data successfully created
            url:x:@.arguments/*/url
            type:x:@.arguments/*/type

         // Checking if caller wants us to execute some lambda object once we're done.
         if
            exists:x:@.arguments/*/.onafter
            .lambda
               eval:x:@.arguments/*/.onafter

      .catch

         // Oops ...!!
         log.error:x:@.arguments/*/message
            url:x:@.arguments/*/url

         // Signaling frontend.
         strings.concat
            .:"Error - "
            get-value:x:@.arguments/*/message
         unwrap:x:+/*/args/*
         sockets.signal:magic.backend.chatbot
            roles:root
            args
               message:x:@strings.concat
               type:error
