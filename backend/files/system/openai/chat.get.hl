
// OpenAI chat endpoint, for having conversations using "gpt-xxx" models.
.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   session:string
.description:OpenAI chat endpoint, for having conversations using "gpt-xxx" models
.type:public

// Invoking slot containing commonalities for all endpoints.
insert-before:x:../*/signal/*/.callback
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Figuring out max tokens for model.
      .model-max-tokens
      .context-max
      switch:x:@.arguments/*/model

         case:gpt-3.5-turbo
         case:gpt-3.5-turbo-0301
            set-value:x:@.model-max-tokens
               .:int:4096
            set-value:x:@.context-max
               .:int:1000

         case:gpt-4
         case:gpt-4-0314
            set-value:x:@.model-max-tokens
               .:int:8192
            set-value:x:@.context-max
               .:int:2000

         case:gpt-4-32k
         case:gpt-4-32k-0314
            set-value:x:@.model-max-tokens
               .:int:32768
            set-value:x:@.context-max
               .:int:4000

         default

            log.error:Invoked chat endpoint with wrong model
            throw:The chat endpoint only supports GPT type of models
               public:bool:true
               status:int:400

      // Retrieving relevant snippets.
      unwrap:x:+/*
      signal:magic.ai.get-context
         type:x:@.arguments/*/type
         prompt:x:@.arguments/*/prompt
         threshold:x:@.arguments/*/threshold
         max_tokens:x:@.context-max
         vector_model:x:@.arguments/*/vector_model
      lambda2hyper:x:@signal
      log.info:x:-

      // Taken from either training data or previous questions.
      .session

      // Checking if question triggered training data.
      if
         exists:x:@signal/*/snippets/*
         .lambda

            // Question triggered training data.
            unwrap:x:+/*/*/*/content
            add:x:@.session
               .
                  .
                     role:user
                     content:x:@signal/*/context

      else

         // Question didn't kick in training data, hence checking if we've got a session.
         cache.get:x:@.arguments/*/session
         if
            not-null:x:@cache.get
            .lambda

               /*
                * We have an existing session, hence continuing discussing subject
                * unless embeddings returns a better match.
                */
               add:x:@.session
                  hyper2lambda:x:@cache.get

      // Checking if model is prefixed, at which point we prepend prefix to question.
      if
         and
            exists:x:@.argument/*/prefix
            not-null:x:@.arguments/*/prefix
            neq:x:@.arguments/*/prefix
               .:
         .lambda

            // Prefixing prompt.
            set-value:x:@.arguments/*/prompt
               strings.concat
                  get-value:x:@.arguments/*/prefix
                  get-value:x:@.arguments/*/prompt

      // Adding user's current question to session.
      unwrap:x:+/*/*/*/content
      add:x:@.session
         .
            .
               role:user
               content:x:@.arguments/*/prompt

      // Figuring out max tokens to ask for, which is max_tokens of model minus size of context.
      lambda2hyper:x:@.session/*
      .max-tokens
      set-value:x:@.max-tokens
         math.subtract
            get-value:x:@.model-max-tokens
            openai.tokenize:x:@lambda2hyper

      // Retrieving token used to invoke OpenAI.
      .token
      set-value:x:@.token
         strings.concat
            .:"Bearer "
            config.get:"magic:openai:key"

      // Invoking OpenAI now with context being either training data or previous messages.
      add:x:./*/http.post/*/payload/*/messages
         get-nodes:x:@.session/*
      unwrap:x:./*/http.post/**
      http.post:"https://api.openai.com/v1/chat/completions"
         headers
            Authorization:x:@.token
            Content-Type:application/json
         payload
            model:x:@.arguments/*/model
            max_tokens:x:@.max-tokens
            temperature:x:@.arguments/*/temperature
            messages
         convert:true

      // Sanity checking above invocation
      if
         not
            and
               mte:x:@http.post
                  .:int:200
               lt:x:@http.post
                  .:int:300
         .lambda

            // Oops, error - Logging error and returning status 500 to caller.
            lambda2hyper:x:@http.post
            log.error:Something went wrong while invoking OpenAI
               message:x:@http.post/*/content/*/error/*/message
               status:x:@http.post
               error:x:@lambda2hyper
            throw:x:@http.post/*/content/*/error/*/message
               public:bool:true
               status:x:@http.post

      else

         // Success! Logging as such!
         log.info:Invoking OpenAI was a success

      // Making sure we trim response.
      .result
      set-value:x:@.result
         strings.trim:x:@http.post/*/content/*/choices/0/*/message/*/content
      get-first-value
         get-value:x:@http.post/*/content/*/choices/0/*/message/*/finish_reason
         .:unknown

      // Adding current prompt to session.
      unwrap:x:+/*/*/*/content
      add:x:@.session
         .
            .
               role:system
               content:x:@.result
      lambda2hyper:x:@.session/*
      cache.set:x:@.arguments/*/session
         expiration:600
         value:x:@lambda2hyper

      // Checking if we've got references.
      if
         and
            exists:x:@.arguments/*/references
            not-null:x:@.arguments/*/references
            get-value:x:@.arguments/*/references
            exists:x:@signal/*/snippets
         .lambda
            add:x:@.callback/*/return
               .
                  references
            add:x:@.callback/*/return/*/references
               get-nodes:x:@signal/*/snippets/*

      /*
       * Applying some HTTP caching to avoid invoking OpenAI again with
       * the same question before some minimum amount of time has passed.
       */
      response.headers.set
         Cache-Control:max-age=30

      // Returning results returned from invocation above to caller.
      unwrap:x:+/*
      return
         result:x:@.result
         finish_reason:x:@get-first-value

// Returning result of worker slot to caller.
return-nodes:x:@signal/*
