
// Invokes OpenAI to find a vector from the specified prompt.
.token
set-value:x:@.token
   strings.concat
      .:"Bearer "
      config.get:"magic:openai:key"

// Associating request with user, if applicable.
auth.ticket.get
if
   not-null:x:@auth.ticket.get
   .lambda

      // Associating request with username.
      unwrap:x:+/*/*
      add:x:@../*/http.post/*/payload
         .
            user:x:@auth.ticket.get

// Creating an HTTP POST request towards OpenAI, now decorated.
http.post:"https://api.openai.com/v1/embeddings"
   headers
      Authorization:x:@.token
      Content-Type:application/json
   payload
      input:x:@.arguments/*/prompt
      model:x:@.arguments/*/vector_model
   convert:true

// Sanity checking above invocation
if
   not
      and
         mte:x:@http.post
            .:int:200
         lt:x:@http.post
            .:int:300
   .lambda

      // Oops, error - Logging error and returning status 500 to caller.
      lambda2hyper:x:@http.post
      log.error:Something went wrong while invoking OpenAI
         message:x:@http.post/*/content/*/error/*/message
         error:x:@lambda2hyper
      response.status.set:x:@http.post
      unwrap:x:+/*
      return
         error:bool:true
         result:x:@http.post/*/content/*/error/*/message

/*
 * Looping through each vector in model, finding the most relevant snippets.
 *
 * Notice, this will calculate the dot product from the question and all training snippets,
 * and use the training snippets with the highest dot product as "the context from which to answer the question".
 *
 * Resulting in that we provide relevant information to OpenAI, allowing them to correctly assemble
 * a completion, based upon the relevant information (context) and the way the question was phrased.
 */
.snippets
.continue:bool:true
.offset:int:0
while:x:@.continue

   // Selecting 50 snippets from type.
   data.read
      table:ml_training_snippets
      columns
         id
         prompt
         completion
         embedding
         uri
      where
         and
            embedding.neq
            type.eq:x:@.arguments/*/type
      limit:50
      offset:x:@.offset
      order:created

   // Looping through all records returned above.
   for-each:x:@data.read/*

      // Calculating dot product of currently iterated snippet and [.vector].
      strings.split:x:@.dp/#/*/embedding
         .:,
      math.dot
         get-nodes:x:@strings.split/*
         get-nodes:x:@http.post/*/content/*/data/0/*/embedding/*

      // Adding result to above [.snippets] collection.
      unwrap:x:+/*/*/*
      add:x:@.snippets
         .
            .
               uri:x:@.dp/#/*/uri
               prompt:x:@.dp/#/*/prompt
               completion:x:@.dp/#/*/completion
               dot:x:@math.dot

   // Sorting [.snippets] such that highest dot product comes first.
   sort:x:@.snippets/*
      if
         mt
            get-value:x:@.lhs/#/*/dot
            get-value:x:@.rhs/#/*/dot
         .lambda
            set-value:x:@.result
               .:int:-1
      else-if
         lt
            get-value:x:@.lhs/#/*/dot
            get-value:x:@.rhs/#/*/dot
         .lambda
            set-value:x:@.result
               .:int:1
      else
         set-value:x:@.result
            .:int:0

   // Updating [.snippets] now with the 50 top most relevant snippets.
   remove-nodes:x:@.snippets/*
   add:x:@.snippets
      get-nodes:x:@sort/*/[0,50]

   // Checking if we're done.
   if
      lt
         get-count:x:@data.read/*
         .:int:50
      .lambda

         // We're done! We've found the top 50 most relevant snippets from our training material.
         set-value:x:@.continue
            .:bool:false
   
   // Incrementing offset.
   math.increment:x:@.offset
      get-count:x:@data.read/*

/*
 * Now we have our most relevant snippets and we can use these to generate an answer
 * to whatever question the caller asked, by doing a little bit of "AI trickery".
 *
 * However, in case user asked a question that's completely irrelevant to the subject
 * of our training snippets, we pass the question to OpenAI using
 * the default model, and just keeping the question "as is".
 */
if
   and
      lt:x:@.snippets/0/*/dot
         convert:x:@.arguments/*/threshold
            type:double
      get-value:x:@.arguments/*/chat
   .lambda

      /*
       * Semantic search resulted in zero high quality snippets,
       * invoking completion with question without context.
       */
      add:x:./*/io.file.execute
         get-nodes:x:@.arguments/*
      io.file.execute:/system/openai/prompt.implementation/completion-prompt.hl
      return-nodes:x:@io.file.execute/*

/*
 * We had at least one training snippet giving us a match on question,
 * hence we make sure cached messages are deleted and treat this question
 * as a new context.
 */
if
   not-null:x:@.arguments/*/session
   .lambda
      cache.set:x:@.arguments/*/session

/*
 * Now we know which snippets are relevant, and we can generate our prompt, by creating a context
 * that we pass on to OpenAI to help it answer the question based upon the snippets matching
 * the question.
 */
.context

// URL of pages used to generate result.
.above

// Tracking that this is new context logic.
.new-context:bool:false

// Checking if we've got a prefix at which point our question changes.
if
   and
      exists:x:@.arguments/*/prefix
      not-null:x:@.arguments/*/prefix
      strings.contains:x:@.arguments/*/prefix
         .:[CONTEXT]
      strings.contains:x:@.arguments/*/prefix
         .:[QUESTION]
   .lambda

      // Prefixed model new style, making sure we use prefix correctly.
      set-value:x:@.context
         get-value:x:@.arguments/*/prefix
      set-value:x:@.new-context
         .:bool:true

else-if
   and
      exists:x:@.arguments/*/prefix
      not-null:x:@.arguments/*/prefix
   .lambda

      // Prefixed model, making sure we use prefix.
      set-value:x:@.context
         strings.concat
            get-value:x:@.arguments/*/prefix
            .:"\r\n\r\nQUESTION: "

else

   // No prefix.
   set-value:x:@.context
      .:"Answer the following QUESTION given the following CONTEXT and preserve Markdown.\r\n\r\nQUESTION: [QUESTION]\r\n\r\nCONTEXT: [CONTEXT]"
   set-value:x:@.new-context
      .:bool:true

// Removing prefix since we have manually create it.
set-value:x:@.arguments/*/prefix
   .:

if:x:@.new-context

   // Creating our "context".
   set-value:x:@.context
      strings.replace:x:@.context
         .:[QUESTION]
         get-value:x:@.arguments/*/prompt

else

   // Creating our "context".
   set-value:x:@.context
      strings.concat
         get-value:x:@.context
         get-value:x:@.arguments/*/prompt
         .:"\r\n\r\n"
         .:"CONTEXT: "

/*
 * Figuring out maximum length of context.
 *
 * Maximum length of context is the number of tokens the model can handle,
 * subtracting the number of tokens the model requests.
 */
.max-context-token-count
switch:x:@.arguments/*/model

   case:text-davinci-003

      // No messages, so here we can use whatever we've got.
      set-value:x:@.max-context-token-count
         math.subtract
            .:int:4096
            .:int:15
            convert:x:@.arguments/*/max_tokens
               type:int

   case:gpt-3.5-turbo
   case:gpt-3.5-turbo-0301

      /*
       * Notice, for chat completion endpoints we keep 1,000 tokens as
       * a reserve for previous messages.
       */
      set-value:x:@.max-context-token-count
         math.divide
            math.subtract
               .:int:4096
               .:int:15
               convert:x:@.arguments/*/max_tokens
                  type:int
            .:int:2

   case:text-davinci-002
   case:code-davinci-002

      set-value:x:@.max-context-token-count
         math.subtract
            .:int:4000
            .:int:15
            convert:x:@.arguments/*/max_tokens
               type:int

   default

      set-value:x:@.max-context-token-count
         math.subtract
            .:int:2048
            .:int:15
            convert:x:@.arguments/*/max_tokens
               type:int

// Looping through all snippets to create our context.
.aggregated-context:
for-each:x:@.snippets/*

   /*
    * Making sure our prompt's complete length never exceeds maximum characters,
    * and that we only use relevant snippets above specified threshold from our model.
    */
   strings.concat
      get-value:x:@.context
      get-value:x:@.aggregated-context
      .:"\r\n\r\n"
      get-value:x:@.dp/#/*/prompt
      .:"\r\n\r\n"
      get-value:x:@.dp/#/*/completion
   if
      and
         mt:x:@.dp/#/*/dot
            convert:x:@.arguments/*/threshold
               type:double
         lt
            openai.tokenize:x:@strings.concat
            get-value:x:@.max-context-token-count
      .lambda

         // We still haven't reached our character threshold yet, and we still have relevant snippets.
         if:x:@.new-context
            set-value:x:@.aggregated-context
               strings.concat
                  get-value:x:@.aggregated-context
                  .:" "
                  get-value:x:@.dp/#/*/prompt
                  .:" "
                  get-value:x:@.dp/#/*/completion
         else
            set-value:x:@.context
               get-value:x:@strings.concat

         // To support old-style URIs, we need to do some trickery here.
         .uri
         set-value:x:@.uri
            get-value:x:@.dp/#/*/uri
         if
            and
               exists:x:@.dp/#/*/uri
               not-null:x:@.dp/#/*/uri
               strings.contains:x:@.dp/#/*/uri
                  .:"://"
            .lambda

               // URL contains "https://" part or "http://" part.
               strings.split:x:@.dp/#/*/uri
                  .:"://"
               if
                  not
                     strings.contains:x:@strings.split/0/-
                        .:"localhost:"
                  .lambda
                     strings.split:x:@strings.split/0/-
                        .:":"
                     set-value:x:@.uri
                        strings.concat
                           get-value:x:@strings.split/@strings.split/0
                           .:"://"
                           get-value:x:@strings.split/0
               else
                  set-value:x:@.uri
                     strings.concat
                        get-value:x:@strings.split/0
                        .:"://"
                        get-value:x:@strings.split/1

         // Checking if we need to store URI as reference.
         if
            and
               not-null:x:@.uri
               not-exists:x:@.above/*/*/uri/={@.uri}
            .lambda

               // Adding reference to URL to [.above] collection.
               unwrap:x:+/*/*/*
               add:x:@.above
                  .
                     .
                        uri:x:@.uri
                        prompt:x:@.dp/#/*/prompt

// Checking if this is our new context logic or not.
if:x:@.new-context

   /*
    * Checking if we're engaging in a "conversation" or not,
    * implying if user asked a follow up question or if we have a new context
    * to accommodate for.
    */
   if
      neq:x:@.aggregated-context
         .:
      .lambda

         // Using context.
         set-value:x:@.context
            strings.replace:x:@.context
               .:[CONTEXT]
               get-value:x:@.aggregated-context
   else

      // Follow up question.
      set-value:x:@.context
         get-value:x:@.arguments/*/prompt
   
else

   // Making sure OpenAI doesn't return "Answer:" to us.
   set-value:x:@.context
      strings.concat
         get-value:x:@.context
         .:"\r\nANSWER: "

// Verifying that user actually wants to have a completion/chat response.
if
   or
      not-exists:x:@.arguments/*/chat
      get-value:x:@.arguments/*/chat
   .lambda

      // Invoking default prompt completion.
      add:x:./*/io.file.execute
         get-nodes:x:@.arguments/*
      set-value:x:./*/io.file.execute/*/prompt
         get-value:x:@.context
      io.file.execute:/system/openai/prompt.implementation/completion-prompt.hl
      add:x:../*/return-nodes
         get-nodes:x:@io.file.execute/*

// Making sure we return URIs if caller asked for it.
if
   and
      exists:x:@.arguments/*/references
      get-value:x:@.arguments/*/references
   .lambda
      add:x:+/*/*
         get-nodes:x:@.above/*
      add:x:../*/return-nodes
         .
            references

// Returning result of above invocation.
return-nodes
